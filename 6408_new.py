# -*- coding: utf-8 -*-
"""6408_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/102-khYG2TExI_Azt4dhVNPd773nmWSFt
"""

import pandas as pd
df = pd.read_csv('colours (1).csv')
df.head()

df['Category'] = df['Category'].str.lower()
df.head()

df.drop(['Sub_category_list'], axis = 1)

unique_labels = set(df['Global_category'])
print('Number of unique labels:', len(unique_labels))

from collections import Counter
Sub_category_count = Counter(df['Global_category']).most_common()

print('15 most frequent Category:')
print(*Sub_category_count[:15], sep = "\n")
print('\n 15 least frequent Category:')
print(*Sub_category_count[-15:], sep = "\n")

unique_labels = set(df['Possible_category'])
print('Number of unique labels:', len(unique_labels))

from collections import Counter
Sub_category_count = Counter(df['Possible_category']).most_common()

print('15 most frequent Category:')
print(*Sub_category_count[:15], sep = "\n")
print('\n 15 least frequent Category:')
print(*Sub_category_count[-15:], sep = "\n")

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

vc = df['Global_category'].value_counts()

## Change the "kind" parameter to "bar" if you prefer a histogram
vc.plot(kind='bar', colormap='Spectral_r', figsize=(6, 6))

#created a dataframe that only contains the sub category and the color of item
color = df[['Sub_category', 'color_index']]
color.head()

color.info()

import nltk
nltk.download('wordnet')

import nltk
nltk.download('stopwords')

import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
print('Original Text: ' + str(df['Sub_category'][2]))
print('\n')
#create an empty mapping table from the str object to strip punctuation from the words
punc = str.maketrans('', '', string.punctuation)
#apply the empty mapping table to each element of the series where x is the review for one document.
df['Sub_category'] = df['Sub_category'].apply(lambda x : ' '.join(word.translate(punc) for word in x.split()))
print('Punctuation Remove: ' + str(df['Sub_category'][2]))
print('\n')
#list of stop words
stop = stopwords.words('english')
#removing the stop words
df['Sub_category'] = df['Sub_category'].apply(lambda x : ' '.join(word for word in x.split() if word not in stop))
print('Stopwords Remove: ' + str(df['Sub_category'][2]))
print('\n')

#Lemmatize words to reduce them to their root form. Note: added the pos = 'v' to reduce the incoming word to verb root
lem = WordNetLemmatizer()
df['Sub_category'] = df['Sub_category'].apply(lambda x : ' '.join(lem.lemmatize(word, pos = 'v') for word in x.split()))
print('Lemmatized Text: ' + str(df['Sub_category'][2]))

import re
import nltk
from nltk.stem.porter import PorterStemmer
english_stemmer=nltk.stem.SnowballStemmer('english')
def data_clean( rev, remove_stopwords=True): 
    

    new_text = re.sub("[^a-zA-Z]"," ", rev)
   
    words = new_text.lower().split()
    
    if remove_stopwords:
        sts = set(stopwords.words("english"))
        words = [w for w in words if not w in sts]
    ary=[]
    eng_stemmer = english_stemmer 
    for word in words:
        ary.append(eng_stemmer.stem(word))

    
    return(ary)

clean_category = []
for rev in df['Sub_category']:
    clean_category.append( " ".join(data_clean(rev)))

#separating the ratings to different colors 
r1 = color[color.color_index.isin([1,2,3,4,5,6,7])]
r0 = color[color.color_index.isin([8,9,10,11,12,13,14,15])]
r1.loc[:, 'color_index'] = 1
r0.loc[:, 'color_index'] = 0

#concat the two new dataframes return one dataframe with preprocessed text and their corresponding labels
rev = pd.concat([r1,r0])
rev.head()

rev.head()
reviews1 = rev['Sub_category']
rev['Sub_category']=rev['Sub_category'].apply(str)
print(reviews1)

Most_used_Words =pd.Series(' '.join(clean_category).lower().split()).value_counts()[:2000]
print (Most_used_Words)

from sklearn.feature_extraction.text import TfidfVectorizer
text_vectorizer = TfidfVectorizer(min_df=4, max_features = 1000)
test_vector = text_vectorizer.fit_transform(clean_category)
tfidf_vector = dict(zip(text_vectorizer.get_feature_names(), text_vectorizer.idf_))

test_categoryText = df.Sub_category
test_category = df.color_index
text_vectorizer = TfidfVectorizer(max_df=.8)
text_vectorizer.fit(test_categoryText)

test_categoryText =test_categoryText[:2000]
test_categoryText

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(min_df = 10)
train_bow = vectorizer.fit_transform(test_categoryText)
print("Shape of train matrix after BOW : ",train_bow.shape)

test_category = test_category[:2000]
test_category

import numpy as np
X = text_vectorizer.transform(test_categoryText).toarray()
y = test_category.values

seed = 123
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2, random_state = seed)

y_train = np.asarray(test_category).astype('float32').reshape((-1,1))
y_test = np.asarray(test_category).astype('float32').reshape((-1,1))

from keras.preprocessing.text import Tokenizer
tokenize = Tokenizer(num_words=1000)
tokenize.fit_on_texts(test_categoryText)

X_train = tokenize.texts_to_sequences(test_categoryText)
X_test = tokenize.texts_to_sequences(test_categoryText)

vocab_size = len(tokenize.word_index) + 1  

print(test_categoryText[2])
print(X_train[2])

maxlen = 100
from keras.preprocessing.sequence import pad_sequences
X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

print(X_train[0, :])

print(X_train.shape)
print(y_train.shape)

from __future__ import division, print_function
from gensim import models
from keras.callbacks import ModelCheckpoint
from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, MaxPooling1D, Embedding, GlobalMaxPool1D
from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Model
from keras.models import Sequential
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
import os
import collections
import re
import string

embedding_dim = 128

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))
model.add(Conv1D(125, 5, activation='relu'))
model.add(GlobalMaxPool1D())
model.add(Dense(100, activation='relu'))
model.add(Dense(10, activation='softmax'))
model.add(Flatten())
model.compile(optimizer='adam',
               loss='binary_crossentropy',
               metrics=['accuracy'])

model.summary()

history = model.fit(X_train, y_train,
                       batch_size=64,
                       epochs=20,
                       validation_data=(X_test, y_test))

import matplotlib.pyplot as plt
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#created a dataframe that only contains the sub category and the color of item
colors = df[['Possible_category', 'color', 'Category']]
colors.head()

y = df['color']
X = df[['Possible_category', 'Category']]

# TODO: Import KMeans
from sklearn.cluster import KMeans

# TODO: Create an instance of KMeans to find two clusters
kmeans_1 = KMeans(n_clusters=2, random_state=0)

# TODO: use fit_predict to cluster the dataset
predictions = kmeans_1.fit_predict(colors)

# Plot
helper.draw_clusters(biased_dataset, predictions)